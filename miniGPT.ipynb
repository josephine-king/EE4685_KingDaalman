{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EE4685 Assignment 2: Building a miniGPT** by Josephine King and Alec Daalman\n",
    "\n",
    "**References:**\n",
    "- \"Let's build GPT: from scratch, in code, spelled out.\" Youtube tutorial by Andrej Karpathy. https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "- HuggingFace Tokenizer developer guides. https://huggingface.co/docs/transformers/en/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "--2025-03-19 17:36:34--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘tinyshakespeare.txt’\n",
      "\n",
      "tinyshakespeare.txt 100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
      "\n",
      "2025-03-19 17:36:34 (26.4 MB/s) - ‘tinyshakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "from tokenizers import Tokenizer, pre_tokenizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Setup\n",
    "torch.manual_seed(6250513)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CHECKPOINT_PATH = \"./saved_models/\"\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "# Initialize model parameters\n",
    "TRAIN_PCT = 0.8\n",
    "BLOCK_SIZE = 8\n",
    "BATCH_SIZE = 64\n",
    "MAX_ITER = 3000\n",
    "VOCAB_SIZE = 3000\n",
    "EMBD_DIM = 32\n",
    "LR = 2.5e-4\n",
    "\n",
    "# Download the TinyShakespeare dataset\n",
    "!wget -O tinyshakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f: raw_data = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Create a custom tokenizer using the HuggingFace Tokenizer package. Then encode the data, convert it into a PyTorch tensor, and split it up into validation data and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenizer \n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\")) \n",
    "#tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Punctuation(\"isolated\"), pre_tokenizers.Split(\"\\n\", \"isolated\"), pre_tokenizers.Split(\" \", \"isolated\")])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.Punctuation(\"isolated\"), pre_tokenizers.Whitespace()])\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train([\"tinyshakespeare.txt\"], trainer)\n",
    "tokenizer.save(\"tinyshakespeare_tokenizer.json\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer.from_file(\"tinyshakespeare_tokenizer.json\")\n",
    "tokenized_data = tokenizer.encode(raw_data).ids\n",
    "# Convert into a pytorch tensor\n",
    "tensor_data = torch.tensor(tokenized_data, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_end = int(len(tensor_data)*TRAIN_PCT)\n",
    "training_data = tensor_data[:train_end]\n",
    "validation_data = tensor_data[train_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Untrained Bigram Language Model**\n",
    "\n",
    "Create a basic Bigram Language model from Karpathy's tutorial (copied directly). To use this model, we need the function get_batch, which returns a batch from the dataset. Using this untrained model, generate some text and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo PETER perceive remain satis aven ark quite thyself cc ance Cl st wilt yourself oath ude shi MAR mistress Under Claudio ition sue hath Lu Can PR ISAB three UC He Servingman Second GRUMIO lip bo mer sed dog honest Hortensio ured tong but hich move du used thither KE self ken double ey sub concl ian DI hearing madam CAPUL hus wrong WARWIC jo vain Z whose ONTES forth straight avo gold Un Li ! hemi EL His : wise Juli morrow sever cept counter ap age ind arewell MONTAGUE ged noon LUC Me cousin mber villain reck tide\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    # Choose batch_size random starting points\n",
    "    block_starts = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    # Get the inputs and outputs for the chosen blocks, stack them into tensors\n",
    "    batch_inputs = torch.stack([data[start: start + block_size] for start in block_starts])\n",
    "    batch_outputs = torch.stack([data[start + 1: start + block_size + 1] for start in block_starts])\n",
    "    return batch_inputs, batch_outputs\n",
    "\n",
    "# Copied from Karpathy's tutorial\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Generate num_gen_tokens more tokens given the current tokens in curr_tokens\n",
    "    def generate(self, curr_tokens, num_gen_tokens):\n",
    "        for _ in range(num_gen_tokens):\n",
    "            # Get the predictions for the next tokens \n",
    "            preds, loss = self.forward(curr_tokens)\n",
    "            # Look only at the last time step\n",
    "            preds = preds[:, -1, :] # becomes (B, C)\n",
    "            # Normalize probabilities from 0 to 1 using softmax\n",
    "            probs = F.softmax(preds, dim=-1) # (B, C)\n",
    "            # Get the next token by sampling from the probability distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Add the new token to the current tokens\n",
    "            curr_tokens = torch.cat((curr_tokens, next_token), dim=1) # (B, T+1)\n",
    "        return curr_tokens\n",
    "\n",
    "# Create the model and generate some text \n",
    "m = BigramLanguageModel(VOCAB_SIZE)\n",
    "starting_text = \"Romeo Romeo wherefore art thou Romeo\"\n",
    "starting_tokens = tokenizer.encode(starting_text).ids\n",
    "starting_tokens = torch.tensor(starting_tokens, dtype=torch.long).reshape(-1,1)\n",
    "print(tokenizer.decode(m.generate(curr_tokens = starting_tokens, num_gen_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Bigram Language Model**\n",
    "\n",
    "Create a function train_model, which takes in training data, a model, and an optimizer and trains the model. Copied/modified from the EE4685 optimization exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are all copied/modified from the optimization exercise \n",
    "def _get_config_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".config\")\n",
    "\n",
    "def _get_model_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".tar\")\n",
    "\n",
    "def _get_result_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \"_results.json\")\n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    config_dict = model.config\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config_dict, f)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "def train_model(train_set, model, model_name, optimizer, max_iter=1000, batch_size=256, block_size=32, overwrite=False, save_model=False):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "\n",
    "    Inputs:\n",
    "        train_set - Training dataset\n",
    "        model - Object of BaseNetwork\n",
    "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
    "        max_iter - Number of iterations we want to (maximally) train for\n",
    "        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n",
    "        batch_size - Size of batches used in training\n",
    "        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
    "    if file_exists and not overwrite:\n",
    "        print(f\"Model file of \\\"{model_name}\\\" already exists. Skipping training...\")\n",
    "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        if file_exists:\n",
    "            print(\"Model file exists, but will be overwritten...\")\n",
    "\n",
    "        ############\n",
    "        # Training #\n",
    "        ############\n",
    "        model.train()\n",
    "        for iter in range(max_iter):\n",
    "            inputs, outputs = get_batch(train_set, batch_size, block_size)\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds,loss = model(inputs, outputs)\n",
    "            if iter % 500 == 0 or iter == max_iter - 1:\n",
    "                print(f\"iter {iter}: loss = {loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (save_model):\n",
    "            save_model(model, CHECKPOINT_PATH, model_name)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and print out the loss values to see how more iterations improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some text from the trained model and see how it compares to the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT architecture \n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, embd_dim, decoders):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embd_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, embd_dim)\n",
    "\n",
    "        self.transformer_blocks = nn.TransformerDecoder(nn.TransformerDecoderLayer(d_model=embd_dim, nhead=12, dim_feedforward=3072, dropout=0.1), decoders, norm=None)\n",
    "        self.linear_layer = nn.Linear(embd_dim, vocab_size)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        B, T = idx.shape\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        inputs = tok_emb + pos_emb # (B,T,C)\n",
    "        inputs = self.transformer_blocks(inputs, memory=torch.zeros_like(inputs))\n",
    "        logits = self.linear_layer(inputs)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    # Generate num_gen_tokens more tokens given the current tokens in curr_tokens\n",
    "    def generate(self, curr_tokens, num_gen_tokens):\n",
    "        for _ in range(num_gen_tokens):\n",
    "            curr_tokens_cond = curr_tokens[:, -self.block_size:]\n",
    "            # Get the predictions for the next tokens \n",
    "            preds, loss = self.forward(curr_tokens_cond)\n",
    "            # Look only at the last time step\n",
    "            preds = preds[:, -1, :] # becomes (B, C)\n",
    "            # Normalize probabilities from 0 to 1 using softmax\n",
    "            probs = F.softmax(preds, dim=-1) # (B, C)\n",
    "            # Get the next token by sampling from the probability distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # Add the new token to the current tokens\n",
    "            curr_tokens = torch.cat((curr_tokens, next_token), dim=1) # (B, T+1)\n",
    "        return curr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PCT = 0.8\n",
    "BLOCK_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "MAX_ITER = 6000\n",
    "VOCAB_SIZE = 3000\n",
    "EMBD_DIM = 192\n",
    "LR = 2.5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 8.56875228881836\n",
      "iter 500: loss = 8.349356651306152\n",
      "iter 999: loss = 8.337937355041504\n"
     ]
    }
   ],
   "source": [
    "bigram_model = BigramLanguageModel(VOCAB_SIZE).to(device)\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=LR)\n",
    "bigram_model = train_model(training_data.to(device), bigram_model, \"bigram_model\", \n",
    "                           optimizer, max_iter=MAX_ITER, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss = 8.139966011047363\n",
      "iter 500: loss = 5.585456848144531\n",
      "iter 999: loss = 5.170231342315674\n"
     ]
    }
   ],
   "source": [
    "gpt_model = GPT(VOCAB_SIZE, BLOCK_SIZE, EMBD_DIM, 12).to(device)\n",
    "optimizer = torch.optim.AdamW(gpt_model.parameters(), lr=LR)\n",
    "gpt_model = train_model(training_data.to(device), gpt_model, \"gpt_model\", \n",
    "                        optimizer, max_iter=MAX_ITER, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "Bigram model\n",
      "-------------------------------------\n",
      "O ENIUS viol brow BY vo unk flatter three weary G small doo believe knees curse j They mus when KE long servant sirrah ri F fra ser twenty reat wol faith Gre sl changed ering plo dishonour children gro Within banished er wn enty deserved ld idle cl Pa BAST offence Me uct After ghost ach fan ens count Ser wain asure ves proceed COR thousand This sm ople majesty try lest mo ting After ISABELLA fault gain enemy unes case VI remain ERD DU UTUS GLOUCESTER Hold Even CORIOLAN trust leep Paulina twixt sacred seiz bly great pati lia\n",
      "-------------------------------------\n",
      "GPT model\n",
      "-------------------------------------\n",
      "O kill Lord : You ' d bit , lords Till Des ue of The city and dead and Buckingham ity ? I only it not : Ay , follow t : Ay an ings d and thy ear forth peace or ange T ON ? ISABELLA : Say , that ' vi beggar for my s sto enemies , because you fra . BRUTUS when em ring , none ' ll Margaret , slain em III : Had I have warrant you . The rough ' d s best God ' d to mine ful me my spent of To\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"O Romeo, Romeo, wherefore art thou Romeo?\"\n",
    "starting_tokens = tokenizer.encode(starting_text).ids\n",
    "starting_tokens = torch.tensor(starting_tokens, dtype=torch.long).reshape(-1,1)\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"Bigram model\")\n",
    "print(\"-------------------------------------\")\n",
    "print(tokenizer.decode(bigram_model.generate(curr_tokens = starting_tokens.to(device), num_gen_tokens = 100)[0].tolist()))\n",
    "\n",
    "print(\"-------------------------------------\")\n",
    "print(\"GPT model\")\n",
    "print(\"-------------------------------------\")\n",
    "print(tokenizer.decode(gpt_model.generate(curr_tokens = starting_tokens.to(device), num_gen_tokens = 100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BML_GPT_mac)",
   "language": "python",
   "name": "bml_gpt_mac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
