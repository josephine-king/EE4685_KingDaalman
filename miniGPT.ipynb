{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EE4685 Assignment 2: Building a miniGPT** by Josephine King and Alec Daalman\n",
    "\n",
    "**References:**\n",
    "- \"Let's build GPT: from scratch, in code, spelled out.\" Youtube tutorial by Andrej Karpathy. https://www.youtube.com/watch?v=kCc8FmEb1nY\n",
    "- HuggingFace Tokenizer developer guides. https://huggingface.co/docs/transformers/en/notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "--2025-03-04 18:01:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘tinyshakespeare.txt’\n",
      "\n",
      "tinyshakespeare.txt 100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-03-04 18:01:30 (30.4 MB/s) - ‘tinyshakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as data\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# Setup\n",
    "torch.manual_seed(6250513)\n",
    "CHECKPOINT_PATH = \"./saved_models/\"\n",
    "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    "print(\"Using device\", device)\n",
    "\n",
    "# Initialize model parameters\n",
    "TRAIN_PCT = 0.8\n",
    "BLOCK_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "MAX_ITER = 5000\n",
    "VOCAB_SIZE = 750\n",
    "LR = 1e-3\n",
    "\n",
    "# Download the TinyShakespeare dataset\n",
    "!wget -O tinyshakespeare.txt https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('tinyshakespeare.txt', 'r', encoding='utf-8') as f: raw_data = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**\n",
    "\n",
    "Create a custom tokenizer using the HuggingFace Tokenizer package. Then encode the data, convert it into a PyTorch tensor, and split it up into validation data and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the tokenizer \n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train([\"tinyshakespeare.txt\"], trainer)\n",
    "tokenizer.save(\"tinyshakespeare_tokenizer.json\")\n",
    "\n",
    "# Tokenize the data\n",
    "tokenizer = Tokenizer.from_file(\"tinyshakespeare_tokenizer.json\")\n",
    "tokenized_data = tokenizer.encode(raw_data).ids\n",
    "# Convert into a pytorch tensor\n",
    "tensor_data = torch.tensor(tokenized_data, dtype=torch.long)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_end = int(len(tensor_data)*TRAIN_PCT)\n",
    "training_data = tensor_data[:train_end]\n",
    "validation_data = tensor_data[train_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Untrained Bigram Language Model**\n",
    "\n",
    "Create a basic Bigram Language model from Karpathy's tutorial (copied directly). To use this model, we need the function get_batch, which returns a batch from the dataset. Using this untrained model, generate some text and see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R ENTIO:\n",
      "  we  ou  be  ugh re T fear VINCENTIO:\n",
      " su ted  th ge  , my   f O fi oul US:\n",
      " \n",
      " ra when ET  her lif then C est ust   than pra the hat  k I s -  them in  this  R thy  Ha per no  are  am lea MI ot  with I have  aw ough and  ARD fear Ed a  e,\n",
      "  that  ell that   th up OM ow  ha  st it  X ol es,\n",
      " ENTIO:\n",
      " Or sir y KING  ain der al ge  to the  , DW ge  ed  ter  LI onour z EDW Which the  ENIUS:\n",
      " vo N TIO:\n",
      " des is sel\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    # Choose batch_size random starting points\n",
    "    block_starts = torch.randint(0, len(data) - block_size, (batch_size,))\n",
    "    # Get the inputs and outputs for the chosen blocks, stack them into tensors\n",
    "    batch_inputs = torch.stack([data[start: start + block_size] for start in block_starts])\n",
    "    batch_outputs = torch.stack([data[start + 1: start + block_size + 1] for start in block_starts])\n",
    "    return batch_inputs, batch_outputs\n",
    "\n",
    "# Copied from Karpathy's tutorial\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "# Create the model and generate some text \n",
    "m = BigramLanguageModel(VOCAB_SIZE)\n",
    "starting_text = \"Romeo Romeo wherefore art thou Romeo\"\n",
    "starting_tokens = tokenizer.encode(starting_text).ids\n",
    "starting_tokens = torch.tensor(starting_tokens, dtype=torch.long).reshape(-1,1)\n",
    "print(tokenizer.decode(m.generate(idx = starting_tokens, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Bigram Language Model**\n",
    "\n",
    "Create a function train_model, which takes in training data, a model, and an optimizer and trains the model. Copied/modified from the EE4685 optimization exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are all copied/modified from the optimization exercise \n",
    "def _get_config_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".config\")\n",
    "\n",
    "def _get_model_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \".tar\")\n",
    "\n",
    "def _get_result_file(model_path, model_name):\n",
    "    return os.path.join(model_path, model_name + \"_results.json\")\n",
    "\n",
    "def save_model(model, model_path, model_name):\n",
    "    config_dict = model.config\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
    "    with open(config_file, \"w\") as f:\n",
    "        json.dump(config_dict, f)\n",
    "    torch.save(model.state_dict(), model_file)\n",
    "\n",
    "def train_model(train_set, model, model_name, optimizer, max_iter=1000, batch_size=256, block_size=32, overwrite=False, save_model=False):\n",
    "    \"\"\"\n",
    "    Train a model on the training set of FashionMNIST\n",
    "\n",
    "    Inputs:\n",
    "        train_set - Training dataset\n",
    "        model - Object of BaseNetwork\n",
    "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
    "        max_iter - Number of iterations we want to (maximally) train for\n",
    "        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n",
    "        batch_size - Size of batches used in training\n",
    "        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
    "    if file_exists and not overwrite:\n",
    "        print(f\"Model file of \\\"{model_name}\\\" already exists. Skipping training...\")\n",
    "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"r\") as f:\n",
    "            results = json.load(f)\n",
    "    else:\n",
    "        if file_exists:\n",
    "            print(\"Model file exists, but will be overwritten...\")\n",
    "\n",
    "        ############\n",
    "        # Training #\n",
    "        ############\n",
    "        model.train()\n",
    "        for iter in range(max_iter):\n",
    "            inputs, outputs = get_batch(train_set, batch_size, block_size)\n",
    "            inputs, outputs = inputs.to(device), outputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds,loss = model(inputs, outputs)\n",
    "            if iter % 500 == 0 or iter == max_iter - 1:\n",
    "                print(f\"iter {iter}: loss = {loss}\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (save_model):\n",
    "            save_model(model, CHECKPOINT_PATH, model_name)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and print out the loss values to see how more iterations improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 7.143737316131592\n",
      "step 500: train loss 6.596506595611572\n",
      "step 1000: train loss 6.02191686630249\n",
      "step 1500: train loss 5.642648696899414\n",
      "step 2000: train loss 5.2434821128845215\n",
      "step 2500: train loss 4.948191165924072\n",
      "step 3000: train loss 4.658974647521973\n",
      "step 3500: train loss 4.507926940917969\n",
      "step 4000: train loss 4.327456474304199\n",
      "step 4500: train loss 4.228084564208984\n",
      "step 4999: train loss 4.095427513122559\n"
     ]
    }
   ],
   "source": [
    "# Create our Bigram model and and Adam optimizer\n",
    "bigram_model = BigramLanguageModel(VOCAB_SIZE)\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters(), lr=LR)\n",
    "bigram_model = train_model(training_data, bigram_model, \"bigram_model\", \n",
    "                            optimizer, max_iter=MAX_ITER, batch_size=BATCH_SIZE, block_size=BLOCK_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some text from the trained model and see how it compares to the untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R ome ap  thy  n ,  Go ese  't ant :\n",
      " S OM ge  su ck ET ain \n",
      " LE sin ,  I  l ow ment s ad had  bl OR D IUS:\n",
      " pla in  your any  think  s o,  you  are  not  had  b ear ful  w o o  was  fi e ver n  t w   hath  hear t\n",
      " \n",
      " C ish 'd  he  ap ,  no  off their  b ack ,  his  m But  I HEN augh t,  li e g er:\n",
      " my  di es  will  fair ,  N ust \n",
      " A  am ong u Which   est\n"
     ]
    }
   ],
   "source": [
    "starting_text = \"Romeo Romeo wherefore art thou Romeo\"\n",
    "starting_tokens = tokenizer.encode(starting_text).ids\n",
    "starting_tokens = torch.tensor(starting_tokens, dtype=torch.long).reshape(-1,1)\n",
    "print(tokenizer.decode(bigram_model.generate(idx = starting_tokens, max_new_tokens=100)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BML_GPT)",
   "language": "python",
   "name": "bml_gpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
